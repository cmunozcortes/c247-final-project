{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Make numpy values easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, initializers, regularizers, models, callbacks\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# fix CUDNN_STATUS_INTERNAL_ERROR\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-guarantee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from 64 to 32 bit floats\n",
    "X_test = np.load(\"/home/alon/Projects/project/X_test.npy\").astype(np.float32)\n",
    "y_test = np.load(\"/home/alon/Projects/project/y_test.npy\").astype(np.float32)\n",
    "person_train_valid = np.load(\"/home/alon/Projects/project/person_train_valid.npy\").astype(np.float32)\n",
    "X_train_valid = np.load(\"/home/alon/Projects/project/X_train_valid.npy\").astype(np.float32)\n",
    "y_train_valid = np.load(\"/home/alon/Projects/project/y_train_valid.npy\").astype(np.float32)\n",
    "person_test = np.load(\"/home/alon/Projects/project/person_test.npy\").astype(np.float32)\n",
    "\n",
    "# adjust labels \n",
    "y_train_valid -= 769\n",
    "y_test -= 769\n",
    "\n",
    "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
    "print ('Test data shape: {}'.format(X_test.shape))\n",
    "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
    "print ('Test target shape: {}'.format(y_test.shape))\n",
    "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
    "print ('Person test shape: {}'.format(person_test.shape))\n",
    "print('y_train_valid', y_train_valid[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-jacksonville",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #import numpy\n",
    "# # x is your dataset\n",
    "# #x = numpy.random.rand(100, 5)\n",
    "# indices = numpy.random.permutation(x.shape[0])\n",
    "# training_idx, test_idx = indices[:80], indices[80:]\n",
    "# training, test = x[training_idx,:], x[test_idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-coordinate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training and validation data:\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.2, random_state=42)\n",
    "#X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.2, random_state=42)\n",
    "\n",
    "print ('Training/Valid data shape: {}'.format(X_train.shape))\n",
    "print ('Validation data shape: {}'.format(X_valid.shape))\n",
    "print ('Training/Valid target shape: {}'.format(y_train.shape))\n",
    "print ('Validation target shape: {}'.format(y_valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ksquare(x):\n",
    "    return tf.pow(x, 2)\n",
    "\n",
    "def klog(x):\n",
    "    return tf.math.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-corruption",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(in_arr):\n",
    "    in_arr = in_arr.reshape((in_arr.shape[0],))\n",
    "    in_arr = in_arr.astype(int)\n",
    "    in_arr_1h = np.zeros((in_arr.size, in_arr.max()+1))\n",
    "    in_arr_1h[np.arange(in_arr.size),in_arr] = 1\n",
    "    return in_arr_1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert all vectors to one-hot\n",
    "y_train_valid_1h = convert_to_one_hot(y_train_valid)\n",
    "y_train_1h = convert_to_one_hot(y_train)\n",
    "y_valid_1h = convert_to_one_hot(y_valid)\n",
    "y_test_1h = convert_to_one_hot(y_test)\n",
    "\n",
    "person_train_valid_1h = convert_to_one_hot(person_train_valid)\n",
    "person_test_1h = convert_to_one_hot(person_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing and then adding noise to every single training input\n",
    "\n",
    "X_train_valid_norm = np.zeros_like(X_train_valid)\n",
    "\n",
    "for eeg_ix in range(X_train_valid.shape[0]):\n",
    "    for i in range(22):\n",
    "        #normalize the data\n",
    "        y = X_train_valid[eeg_ix,i,:]\n",
    "        mean = np.mean(y)\n",
    "        std = np.std(y)\n",
    "        z = (y-mean)/std\n",
    "        X_train_valid_norm[eeg_ix,i,:] = z\n",
    "        \n",
    "\n",
    "#don't want to actually use the one with noise        \n",
    "# mu, sigma = 0, 0.10\n",
    "# s = np.random.normal(mu, sigma, X_train_valid_norm.shape)\n",
    "# print(\"Noise shape: {}\".format(s.shape))\n",
    "# X_train_valid_norm_noised = X_train_valid_norm + s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-zealand",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing and then adding noise to every single training input\n",
    "\n",
    "X_train_norm = np.zeros_like(X_train)\n",
    "\n",
    "for eeg_ix in range(X_train.shape[0]):\n",
    "    for i in range(22):\n",
    "        #normalize the data\n",
    "        y = X_train[eeg_ix,i,:]\n",
    "        mean = np.mean(y)\n",
    "        std = np.std(y)\n",
    "        z = (y-mean)/std\n",
    "        X_train_norm[eeg_ix,i,:] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-vertical",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation data as well\n",
    "X_valid_norm = np.zeros_like(X_valid)\n",
    "\n",
    "for eeg_ix in range(X_valid.shape[0]):\n",
    "    for i in range(22):\n",
    "        #normalize the data\n",
    "        y = X_valid[eeg_ix,i,:]\n",
    "        mean = np.mean(y)\n",
    "        std = np.std(y)\n",
    "        z = (y-mean)/std\n",
    "        X_valid_norm[eeg_ix,i,:] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing and then adding noise to every single test input\n",
    "\n",
    "X_test_norm = np.zeros_like(X_test)\n",
    "\n",
    "for eeg_ix in range(X_test.shape[0]):\n",
    "    for i in range(22):\n",
    "        #normalize the data\n",
    "        y = X_test[eeg_ix,i,:]\n",
    "        mean = np.mean(y)\n",
    "        std = np.std(y)\n",
    "        z = (y-mean)/std\n",
    "        X_test_norm[eeg_ix,i,:] = z\n",
    "        \n",
    "# mu, sigma = 0, 0.10\n",
    "# s = np.random.normal(mu, sigma, X_test_norm.shape)\n",
    "# print(\"Noise shape: {}\".format(s.shape))\n",
    "# X_test_norm_noised = X_test_norm + s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crop for 12\n",
    "\n",
    "X_train_valid_norm_cropped = X_train_valid_norm[:,:,:500]\n",
    "X_train_norm_cropped = X_train_norm[:,:,:500]\n",
    "X_valid_norm_cropped = X_valid_norm[:,:,:500]\n",
    "X_test_norm_cropped = X_test_norm[:,:,:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-guess",
   "metadata": {},
   "source": [
    "## This is the preprocessing setup for the current best network done: standardized inputs and cropped.\n",
    "\n",
    "## Now let's try dimensionality reduction, first using PCA, and then UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-manor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a look at some random capture\n",
    "\n",
    "fig, axs = plt.subplots(11,2,figsize=(20,40))\n",
    "\n",
    "count = 0\n",
    "\n",
    "eeg_ix = 10\n",
    "\n",
    "for i in range(22):\n",
    "    y = X_train_valid_norm_cropped[eeg_ix,i,:]\n",
    "    \n",
    "    axs[int(i/2),count].plot(y)\n",
    "    axs[int(i/2),count].set_title('EEG Channel {}, standardized and cropped'.format(i+1))\n",
    "    \n",
    "    count += 1\n",
    "    if (count>1):\n",
    "        count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 18\n",
    "\n",
    "#flip\n",
    "X_train_flipped = np.transpose(X_train_norm_cropped, (0,2,1))\n",
    "X_valid_flipped = np.transpose(X_valid_norm_cropped, (0,2,1))\n",
    "X_test_flipped = np.transpose(X_test_norm_cropped, (0,2,1))\n",
    "\n",
    "print(X_train_flipped.shape)\n",
    "print(X_valid_flipped.shape)\n",
    "print(X_test_flipped.shape)\n",
    "\n",
    "X_train_reshaped = X_train_flipped.reshape((X_train_norm_cropped.shape[0]*X_train_norm_cropped.shape[2], 22)) \n",
    "X_valid_reshaped = X_valid_flipped.reshape((X_valid_norm_cropped.shape[0]*X_valid_norm_cropped.shape[2], 22)) \n",
    "X_test_reshaped = X_test_flipped.reshape((X_test_norm_cropped.shape[0]*X_test_norm_cropped.shape[2], 22)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try a PCA on the training data\n",
    "pca = PCA(n_components=n_comp)                  #create the pca object       \n",
    "pca.fit(X_train_reshaped)                               #fit it to your transformed data\n",
    "X_train_transformed=np.empty([X_train_norm_cropped.shape[0],X_train_norm_cropped.shape[2],n_comp])\n",
    "for i in range(len(X_train_flipped)):\n",
    "    #print(X_train_valid_flipped[i].shape)\n",
    "    X_train_transformed[i]=pca.transform(X_train_flipped[i])           #iteratively apply the transformation to each instance of the original dataset\n",
    "\n",
    "print(X_train_transformed.shape)\n",
    "X_train_transformed = np.transpose(X_train_transformed, (0,2,1))\n",
    "print(X_train_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-uganda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now apply that transformation to the validation and test sets\n",
    "X_valid_transformed=np.empty([X_valid_norm_cropped.shape[0],X_valid_norm_cropped.shape[2],n_comp])\n",
    "X_test_transformed=np.empty([X_test_norm_cropped.shape[0],X_test_norm_cropped.shape[2],n_comp])\n",
    "\n",
    "for i in range(len(X_valid_flipped)):\n",
    "    #print(X_train_valid_flipped[i].shape)\n",
    "    X_valid_transformed[i]=pca.transform(X_valid_flipped[i])           #iteratively apply the transformation to each instance of the original dataset\n",
    "\n",
    "#print(X_train_transformed.shape)\n",
    "X_valid_transformed = np.transpose(X_valid_transformed, (0,2,1))\n",
    "print(X_valid_transformed.shape)\n",
    "\n",
    "for i in range(len(X_test_flipped)):\n",
    "    #print(X_train_valid_flipped[i].shape)\n",
    "    X_test_transformed[i]=pca.transform(X_test_flipped[i])           #iteratively apply the transformation to each instance of the original dataset\n",
    "\n",
    "#print(X_train_transformed.shape)\n",
    "X_test_transformed = np.transpose(X_test_transformed, (0,2,1))\n",
    "print(X_test_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "for drp in np.arange(0.7, 0.9, 0.01):\n",
    "    print(drp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-dialogue",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_accuracies_final_1 = []\n",
    "test_accuracies_best_val = []\n",
    "\n",
    "for drp in np.arange(0.7, 0.9, 0.01):\n",
    "    for n_comp in range(6, 22):\n",
    "        print(\"Testing PCA Network for dropout = {} and n_dims = {}\".format(drp,n_comp))\n",
    "\n",
    "        #try a PCA on the data\n",
    "        #reshaped_data = X_train_valid_flipped.reshape((X_train_valid_cropped.shape[1]*X_train_valid_cropped.shape[2], 22))    # create one big data panel with 20 series and 300.000 datapoints\n",
    "                                              #choose the number of features to have after dimensionality reduction\n",
    "        pca = PCA(n_components=n_comp)                  #create the pca object       \n",
    "        pca.fit(X_train_reshaped)                               #fit it to your transformed data\n",
    "        X_train_transformed=np.empty([X_train_norm_cropped.shape[0],X_train_norm_cropped.shape[2],n_comp])\n",
    "        for i in range(len(X_train_flipped)):\n",
    "            #print(X_train_valid_flipped[i].shape)\n",
    "            X_train_transformed[i]=pca.transform(X_train_flipped[i])           #iteratively apply the transformation to each instance of the original dataset\n",
    "\n",
    "\n",
    "        #print(X_train_valid_transformed.shape)    #we end up with examples*timesteps*channels\n",
    "        #now transpose back to examples*channels*timesteps\n",
    "        X_train_transformed = np.transpose(X_train_transformed, (0,2,1))\n",
    "\n",
    "\n",
    "        #now apply that transformation to the validation and test sets\n",
    "        X_valid_transformed=np.empty([X_valid_norm_cropped.shape[0],X_valid_norm_cropped.shape[2],n_comp])\n",
    "        X_test_transformed=np.empty([X_test_norm_cropped.shape[0],X_test_norm_cropped.shape[2],n_comp])\n",
    "\n",
    "        for i in range(len(X_valid_flipped)):\n",
    "            #print(X_train_valid_flipped[i].shape)\n",
    "            X_valid_transformed[i]=pca.transform(X_valid_flipped[i])           #iteratively apply the transformation to each instance of the original dataset\n",
    "\n",
    "        #print(X_train_transformed.shape)\n",
    "        X_valid_transformed = np.transpose(X_valid_transformed, (0,2,1))\n",
    "        #print(X_valid_transformed.shape)\n",
    "\n",
    "        for i in range(len(X_test_flipped)):\n",
    "            #print(X_train_valid_flipped[i].shape)\n",
    "            X_test_transformed[i]=pca.transform(X_test_flipped[i])           #iteratively apply the transformation to each instance of the original dataset\n",
    "\n",
    "        #print(X_train_transformed.shape)\n",
    "        X_test_transformed = np.transpose(X_test_transformed, (0,2,1))\n",
    "\n",
    "        input_ = layers.Input(shape=(n_comp, 500))\n",
    "        r1 = layers.Reshape(target_shape=(n_comp, 500, 1))(input_)\n",
    "        c1 = layers.Conv2D(filters=40, kernel_size=(1,25), data_format='channels_last',\n",
    "                           activation='elu', kernel_regularizer='l2')(r1)\n",
    "        p1 = layers.Permute(dims=(2,1,3))(c1)\n",
    "        r2 = layers.Reshape((476, n_comp*40))(p1)\n",
    "        d1 = layers.Dense(40, activation='elu')(r2)\n",
    "        sq1 = layers.Activation(ksquare)(d1)\n",
    "        ap1 = layers.AveragePooling1D(75, strides=15)(sq1)\n",
    "        log1 = layers.Activation(klog)(ap1)\n",
    "        f1 = layers.Flatten()(log1)\n",
    "        d2 = layers.Dropout(drp)(f1)\n",
    "        output_ = layers.Dense(4, activation='softmax', kernel_regularizer='l2', kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01))(d2)\n",
    "\n",
    "        model = models.Model(inputs=input_, outputs=output_, name='shallow_pca_convnet_one_hot')\n",
    "        #opt = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "        #model.summary()\n",
    "\n",
    "        mcp_save = callbacks.ModelCheckpoint('.mdl_wts_best_acc' + str(n_comp) + '_dims.hdf5', save_best_only=True, monitor='val_acc', mode='max')\n",
    "\n",
    "        loss_hist = model.fit(X_train_transformed, y_train_1h, epochs=150,\n",
    "                          validation_data=(X_valid_transformed, y_valid_1h),\n",
    "                          callbacks=[mcp_save], \n",
    "                          verbose=True)\n",
    "\n",
    "        hist = loss_hist.history\n",
    "\n",
    "        fig, axs = plt.subplots(1,2, figsize=(20, 12))\n",
    "\n",
    "        fig.suptitle(\"Training results for dropout={} and n_dims={}\".format(drp, n_comp), fontsize=14)\n",
    "\n",
    "        axs[0].plot(hist['loss'])\n",
    "        axs[0].plot(hist['val_loss'])\n",
    "        axs[0].set_ylabel('loss')\n",
    "        axs[0].set_xlabel('epoch')\n",
    "        axs[0].legend(['train', 'val'])\n",
    "\n",
    "        axs[1].plot(hist['acc'])\n",
    "        axs[1].plot(hist['val_acc'])\n",
    "        axs[1].set_ylabel('accuracy')\n",
    "        axs[1].set_xlabel('epoch')\n",
    "        axs[1].legend(['train', 'val'])\n",
    "\n",
    "        # Evaluate the model on the test data using `evaluate`\n",
    "        print(\"Evaluate final step on test data\")\n",
    "        results = model.evaluate(X_test_transformed, y_test_1h, batch_size=128)\n",
    "        print(\"final test loss, test acc:\", results)\n",
    "\n",
    "        test_accuracies_final_1.append(results[1])\n",
    "\n",
    "        model.load_weights('.mdl_wts_best_acc' + str(n_comp) + '_dims.hdf5')\n",
    "\n",
    "        # Evaluate the model on the test data using `evaluate`\n",
    "        print(\"Evaluate best val acc model on test data\")\n",
    "        results = model.evaluate(X_test_transformed, y_test_1h, batch_size=128)\n",
    "        print(\"best val acc model test loss, test acc:\", results)\n",
    "\n",
    "        test_accuracies_best_val.append(results[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-yorkshire",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_accuracies_final_2 = []\n",
    "test_accuracies_best_loss = []\n",
    "\n",
    "for drp in np.arange(0.7, 0.9, 0.01):\n",
    "    for n_comp in range(6, 22):\n",
    "        print(\"Testing PCA Network for dropout = {} and n_dims = {}\".format(drp,n_comp))\n",
    "\n",
    "        #try a PCA on the data\n",
    "        #reshaped_data = X_train_valid_flipped.reshape((X_train_valid_cropped.shape[1]*X_train_valid_cropped.shape[2], 22))    # create one big data panel with 20 series and 300.000 datapoints\n",
    "                                              #choose the number of features to have after dimensionality reduction\n",
    "        pca = PCA(n_components=n_comp)                  #create the pca object       \n",
    "        pca.fit(X_train_reshaped)                               #fit it to your transformed data\n",
    "        X_train_transformed=np.empty([X_train_norm_cropped.shape[0],X_train_norm_cropped.shape[2],n_comp])\n",
    "        for i in range(len(X_train_flipped)):\n",
    "            #print(X_train_valid_flipped[i].shape)\n",
    "            X_train_transformed[i]=pca.transform(X_train_flipped[i])           #iteratively apply the transformation to each instance of the original dataset\n",
    "\n",
    "\n",
    "        #print(X_train_valid_transformed.shape)    #we end up with examples*timesteps*channels\n",
    "        #now transpose back to examples*channels*timesteps\n",
    "        X_train_transformed = np.transpose(X_train_transformed, (0,2,1))\n",
    "\n",
    "\n",
    "        #now apply that transformation to the validation and test sets\n",
    "        X_valid_transformed=np.empty([X_valid_norm_cropped.shape[0],X_valid_norm_cropped.shape[2],n_comp])\n",
    "        X_test_transformed=np.empty([X_test_norm_cropped.shape[0],X_test_norm_cropped.shape[2],n_comp])\n",
    "\n",
    "        for i in range(len(X_valid_flipped)):\n",
    "            #print(X_train_valid_flipped[i].shape)\n",
    "            X_valid_transformed[i]=pca.transform(X_valid_flipped[i])           #iteratively apply the transformation to each instance of the original dataset\n",
    "\n",
    "        #print(X_train_transformed.shape)\n",
    "        X_valid_transformed = np.transpose(X_valid_transformed, (0,2,1))\n",
    "        #print(X_valid_transformed.shape)\n",
    "\n",
    "        for i in range(len(X_test_flipped)):\n",
    "            #print(X_train_valid_flipped[i].shape)\n",
    "            X_test_transformed[i]=pca.transform(X_test_flipped[i])           #iteratively apply the transformation to each instance of the original dataset\n",
    "\n",
    "        #print(X_train_transformed.shape)\n",
    "        X_test_transformed = np.transpose(X_test_transformed, (0,2,1))\n",
    "\n",
    "        input_ = layers.Input(shape=(n_comp, 500))\n",
    "        r1 = layers.Reshape(target_shape=(n_comp, 500, 1))(input_)\n",
    "        c1 = layers.Conv2D(filters=40, kernel_size=(1,25), data_format='channels_last',\n",
    "                           activation='elu', kernel_regularizer='l2')(r1)\n",
    "        p1 = layers.Permute(dims=(2,1,3))(c1)\n",
    "        r2 = layers.Reshape((476, n_comp*40))(p1)\n",
    "        d1 = layers.Dense(40, activation='elu')(r2)\n",
    "        sq1 = layers.Activation(ksquare)(d1)\n",
    "        ap1 = layers.AveragePooling1D(75, strides=15)(sq1)\n",
    "        log1 = layers.Activation(klog)(ap1)\n",
    "        f1 = layers.Flatten()(log1)\n",
    "        d2 = layers.Dropout(0.80)(f1)\n",
    "        output_ = layers.Dense(4, activation='softmax', kernel_regularizer='l2', kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01))(d2)\n",
    "\n",
    "        model = models.Model(inputs=input_, outputs=output_, name='shallow_pca_convnet_one_hot')\n",
    "        #opt = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "        #model.summary()\n",
    "\n",
    "        mcp_save = callbacks.ModelCheckpoint('.mdl_wts_best_loss' + str(n_comp) + '_dims.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "        loss_hist = model.fit(X_train_transformed, y_train_1h, epochs=150,\n",
    "                          validation_data=(X_valid_transformed, y_valid_1h),\n",
    "                          callbacks=[mcp_save], \n",
    "                          verbose=True)\n",
    "\n",
    "        hist = loss_hist.history\n",
    "\n",
    "        fig, axs = plt.subplots(1,2, figsize=(20, 12))\n",
    "\n",
    "        fig.suptitle(\"Training results for dropout={} and n_dims={}\".format(drp, n_comp), fontsize=14)\n",
    "\n",
    "        axs[0].plot(hist['loss'])\n",
    "        axs[0].plot(hist['val_loss'])\n",
    "        axs[0].set_ylabel('loss')\n",
    "        axs[0].set_xlabel('epoch')\n",
    "        axs[0].legend(['train', 'val'])\n",
    "\n",
    "        axs[1].plot(hist['acc'])\n",
    "        axs[1].plot(hist['val_acc'])\n",
    "        axs[1].set_ylabel('accuracy')\n",
    "        axs[1].set_xlabel('epoch')\n",
    "        axs[1].legend(['train', 'val'])\n",
    "\n",
    "        # Evaluate the model on the test data using `evaluate`\n",
    "        print(\"Evaluate final step on test data\")\n",
    "        results = model.evaluate(X_test_transformed, y_test_1h, batch_size=128)\n",
    "        print(\"final test loss, test acc:\", results)\n",
    "\n",
    "        test_accuracies_final_2.append(results[1])\n",
    "\n",
    "        model.load_weights('.mdl_wts_best_loss' + str(n_comp) + '_dims.hdf5')\n",
    "\n",
    "        # Evaluate the model on the test data using `evaluate`\n",
    "        print(\"Evaluate best val loss model on test data\")\n",
    "        results = model.evaluate(X_test_transformed, y_test_1h, batch_size=128)\n",
    "        print(\"best val loss model test loss, test acc:\", results)\n",
    "\n",
    "        test_accuracies_best_loss.append(results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-composition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-genome",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-canyon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-dakota",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-stable",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-anatomy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = layers.Input(shape=(n_comp, 500))\n",
    "r1 = layers.Reshape(target_shape=(n_comp, 500, 1))(input_)\n",
    "c1 = layers.Conv2D(filters=40, kernel_size=(1,25), data_format='channels_last',\n",
    "                   activation='elu', kernel_regularizer='l2')(r1)\n",
    "p1 = layers.Permute(dims=(2,1,3))(c1)\n",
    "r2 = layers.Reshape((476, n_comp*40))(p1)\n",
    "d1 = layers.Dense(40, activation='elu')(r2)\n",
    "sq1 = layers.Activation(ksquare)(d1)\n",
    "ap1 = layers.AveragePooling1D(75, strides=15)(sq1)\n",
    "log1 = layers.Activation(klog)(ap1)\n",
    "f1 = layers.Flatten()(log1)\n",
    "d2 = layers.Dropout(0.80)(f1)\n",
    "output_ = layers.Dense(4, activation='softmax', kernel_regularizer='l2', kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01))(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Model(inputs=input_, outputs=output_, name='shallow_convnet_one_hot')\n",
    "#opt = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-elevation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Training 12\n",
    "\n",
    "## Wow, best performance now 74%\n",
    "\n",
    "earlyStopping = callbacks.EarlyStopping(monitor='val_loss', patience=30, verbose=True, restore_best_weights=True, mode='min')\n",
    "mcp_save = callbacks.ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_acc', mode='min')\n",
    "#thresh_cb = MyThresholdCallback(threshold=0.73)\n",
    "#reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "#loss_hist = model.fit(Xtr_more, Ytr_more,epochs=50, verbose=0, callbacks=[earlyStopping, mcp_save, reduce_lr_loss], validation_split=0.25)\n",
    "\n",
    "\n",
    "loss_hist = model.fit(X_train_transformed, y_train_1h, epochs=100,\n",
    "                      validation_data=(X_valid_transformed, y_valid_1h),\n",
    "                      callbacks=[mcp_save], \n",
    "                      verbose=True)\n",
    "\n",
    "hist = loss_hist.history\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(hist['loss'])\n",
    "plt.plot(hist['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(hist['acc'])\n",
    "plt.plot(hist['val_acc'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = loss_hist.history\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(20, 12))\n",
    "\n",
    "fig.suptitle(\"Training results\", fontsize=14)\n",
    "\n",
    "axs[0].plot(hist['loss'])\n",
    "axs[0].plot(hist['val_loss'])\n",
    "axs[0].set_ylabel('loss')\n",
    "axs[0].set_xlabel('epoch')\n",
    "axs[0].legend(['train', 'val'])\n",
    "\n",
    "axs[1].plot(hist['acc'])\n",
    "axs[1].plot(hist['val_acc'])\n",
    "axs[1].set_ylabel('accuracy')\n",
    "axs[1].set_xlabel('epoch')\n",
    "axs[1].legend(['train', 'val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-calvin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('.mdl_wts.hdf5')\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_test_transformed, y_test_1h, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('.mdl_wts.hdf5')\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_test_transformed, y_test_1h, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-introduction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-parallel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-fabric",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
