{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "established-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Make numpy values easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, initializers, regularizers, models, callbacks\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# fix CUDNN_STATUS_INTERNAL_ERROR\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "imposed-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ksquare(x):\n",
    "  return tf.pow(x, 2)\n",
    "\n",
    "def klog(x):\n",
    "  return tf.math.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "guilty-sender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Valid data shape: (2115, 22, 1000)\n",
      "Test data shape: (443, 22, 1000)\n",
      "Training/Valid target shape: (2115,)\n",
      "Test target shape: (443,)\n",
      "Person train/valid shape: (2115, 1)\n",
      "Person test shape: (443, 1)\n",
      "y_train_valid [2. 3. 0. 0. 0. 0. 2. 1. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "# Convert from 64 to 32 bit floats\n",
    "X_test = np.load(\"/home/alon/Projects/project/X_test.npy\").astype(np.float32)\n",
    "y_test = np.load(\"/home/alon/Projects/project/y_test.npy\").astype(np.float32)\n",
    "person_train_valid = np.load(\"/home/alon/Projects/project/person_train_valid.npy\").astype(np.float32)\n",
    "X_train_valid = np.load(\"/home/alon/Projects/project/X_train_valid.npy\").astype(np.float32)\n",
    "y_train_valid = np.load(\"/home/alon/Projects/project/y_train_valid.npy\").astype(np.float32)\n",
    "person_test = np.load(\"/home/alon/Projects/project/person_test.npy\").astype(np.float32)\n",
    "\n",
    "# adjust labels \n",
    "y_train_valid -= 769\n",
    "y_test -= 769\n",
    "\n",
    "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
    "print ('Test data shape: {}'.format(X_test.shape))\n",
    "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
    "print ('Test target shape: {}'.format(y_test.shape))\n",
    "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
    "print ('Person test shape: {}'.format(person_test.shape))\n",
    "print('y_train_valid', y_train_valid[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "southern-ordinary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2 ... 2112 2113 2114]\n",
      "Training/Valid data shape: (1692, 22, 1000)\n",
      "Validation data shape: (423, 22, 1000)\n",
      "Training/Valid target shape: (1692,)\n",
      "Validation target shape: (423,)\n",
      "Training target shape: (1692,)\n",
      "Validation target shape: (423,)\n"
     ]
    }
   ],
   "source": [
    "#get indices\n",
    "indices_train_val = np.arange(0, person_train_valid.shape[0])\n",
    "print(indices_train_val)\n",
    "\n",
    "#split training and validation data:\n",
    "X_train, X_valid, y_train, y_valid, indices_train, indices_val = train_test_split(X_train_valid, y_train_valid, indices_train_val, test_size=0.2, random_state=42)\n",
    "#X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.2, random_state=42)\n",
    "\n",
    "person_train = person_train_valid[indices_train]\n",
    "person_valid = person_train_valid[indices_val]\n",
    "\n",
    "print ('Training/Valid data shape: {}'.format(X_train.shape))\n",
    "print ('Validation data shape: {}'.format(X_valid.shape))\n",
    "print ('Training/Valid target shape: {}'.format(y_train.shape))\n",
    "print ('Validation target shape: {}'.format(y_valid.shape))\n",
    "print ('Training target shape: {}'.format(indices_train.shape))\n",
    "print ('Validation target shape: {}'.format(indices_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "amino-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(in_arr):\n",
    "    in_arr = in_arr.reshape((in_arr.shape[0],))\n",
    "    in_arr = in_arr.astype(int)\n",
    "    in_arr_1h = np.zeros((in_arr.size, in_arr.max()+1))\n",
    "    in_arr_1h[np.arange(in_arr.size),in_arr] = 1\n",
    "    return in_arr_1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "curious-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert all vectors to one-hot\n",
    "y_train_valid_1h = convert_to_one_hot(y_train_valid)\n",
    "y_train_1h = convert_to_one_hot(y_train)\n",
    "y_valid_1h = convert_to_one_hot(y_valid)\n",
    "y_test_1h = convert_to_one_hot(y_test)\n",
    "\n",
    "person_train_valid_1h = convert_to_one_hot(person_train_valid)\n",
    "person_train_1h = convert_to_one_hot(person_train)\n",
    "person_valid_1h = convert_to_one_hot(person_valid)\n",
    "person_test_1h = convert_to_one_hot(person_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #instantiate and initialize pandas dataframes\n",
    "\n",
    "# columns = ['subject_ix', 'subject_one_hot', 'data_22ch', 'event_class_ix', 'event_class_one_hot']\n",
    "\n",
    "# train_val_df = pd.DataFrame(columns=columns)\n",
    "# train_val_df['subject_ix'] = person_train_valid.reshape((person_train_valid.shape[0],))\n",
    "# train_val_df['subject_one_hot'] = person_train_valid_1h.tolist()\n",
    "# train_val_df['event_class_ix'] = y_train_valid\n",
    "# train_val_df['event_class_one_hot'] = y_train_valid_1h.tolist()\n",
    "# train_val_df['data_22ch'] = X_train_valid.tolist()\n",
    "\n",
    "# test_df = pd.DataFrame(columns=columns)\n",
    "# test_df['subject_ix'] = person_test.reshape((person_test.shape[0],))\n",
    "# test_df['subject_one_hot'] = person_test_1h.tolist()\n",
    "# test_df['event_class_ix'] = y_test\n",
    "# test_df['event_class_one_hot'] = y_test.tolist()\n",
    "# test_df['data_22ch'] = X_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rural-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing and then adding noise to every single training input\n",
    "\n",
    "X_train_valid_norm = np.zeros_like(X_train_valid)\n",
    "\n",
    "for eeg_ix in range(X_train_valid.shape[0]):\n",
    "    for i in range(22):\n",
    "        #normalize the data\n",
    "        y = X_train_valid[eeg_ix,i,:]\n",
    "        mean = np.mean(y)\n",
    "        std = np.std(y)\n",
    "        z = (y-mean)/std\n",
    "        X_train_valid_norm[eeg_ix,i,:] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beautiful-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing and then adding noise to every single training input\n",
    "\n",
    "X_train_norm = np.zeros_like(X_train)\n",
    "\n",
    "for eeg_ix in range(X_train.shape[0]):\n",
    "    for i in range(22):\n",
    "        #normalize the data\n",
    "        y = X_train[eeg_ix,i,:]\n",
    "        mean = np.mean(y)\n",
    "        std = np.std(y)\n",
    "        z = (y-mean)/std\n",
    "        X_train_norm[eeg_ix,i,:] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "confident-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation data as well\n",
    "X_valid_norm = np.zeros_like(X_valid)\n",
    "\n",
    "for eeg_ix in range(X_valid.shape[0]):\n",
    "    for i in range(22):\n",
    "        #normalize the data\n",
    "        y = X_valid[eeg_ix,i,:]\n",
    "        mean = np.mean(y)\n",
    "        std = np.std(y)\n",
    "        z = (y-mean)/std\n",
    "        X_valid_norm[eeg_ix,i,:] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "competent-telling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing and then adding noise to every single test input\n",
    "\n",
    "X_test_norm = np.zeros_like(X_test)\n",
    "\n",
    "for eeg_ix in range(X_test.shape[0]):\n",
    "    for i in range(22):\n",
    "        #normalize the data\n",
    "        y = X_test[eeg_ix,i,:]\n",
    "        mean = np.mean(y)\n",
    "        std = np.std(y)\n",
    "        z = (y-mean)/std\n",
    "        X_test_norm[eeg_ix,i,:] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "contained-julian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crop for 12\n",
    "\n",
    "X_train_valid_norm_cropped = X_train_valid_norm[:,:,:500]\n",
    "X_train_norm_cropped = X_train_norm[:,:,:500]\n",
    "X_valid_norm_cropped = X_valid_norm[:,:,:500]\n",
    "X_test_norm_cropped = X_test_norm[:,:,:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fleet-siemens",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'CDF of Actions in Test Set')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#want to see the distribution of actions, subjects\n",
    "plt.figure(figsize=(10,7))\n",
    "actions_hist_train = plt.hist(y_train)\n",
    "plt.title(\"CDF of Actions in Training Set\")\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "actions_hist_train = plt.hist(y_valid)\n",
    "plt.title(\"CDF of Actions in Validation Set\")\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "actions_hist_train = plt.hist(y_test)\n",
    "plt.title(\"CDF of Actions in Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "monthly-uniform",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate and initialize pandas dataframes\n",
    "\n",
    "columns = ['subject_ix', 'subject_one_hot', 'data_22ch', 'event_class_ix', 'event_class_one_hot']\n",
    "\n",
    "train_df = pd.DataFrame(columns=columns)\n",
    "train_df['subject_ix'] = person_train.reshape((person_train.shape[0],))\n",
    "train_df['subject_one_hot'] = person_train_1h.tolist()\n",
    "train_df['event_class_ix'] = y_train\n",
    "train_df['event_class_one_hot'] = y_train_1h.tolist()\n",
    "train_df['data_22ch'] = X_train.tolist()\n",
    "train_df['data_22ch_norm_crop'] = X_train_norm_cropped.tolist()\n",
    "\n",
    "valid_df = pd.DataFrame(columns=columns)\n",
    "valid_df['subject_ix'] = person_valid.reshape((person_valid.shape[0],))\n",
    "valid_df['subject_one_hot'] = person_valid_1h.tolist()\n",
    "valid_df['event_class_ix'] = y_valid\n",
    "valid_df['event_class_one_hot'] = y_valid_1h.tolist()\n",
    "valid_df['data_22ch'] = X_valid.tolist()\n",
    "valid_df['data_22ch_norm_crop'] = X_valid_norm_cropped.tolist()\n",
    "\n",
    "test_df = pd.DataFrame(columns=columns)\n",
    "test_df['subject_ix'] = person_test.reshape((person_test.shape[0],))\n",
    "test_df['subject_one_hot'] = person_test_1h.tolist()\n",
    "test_df['event_class_ix'] = y_test\n",
    "test_df['event_class_one_hot'] = y_test.tolist()\n",
    "test_df['data_22ch'] = X_test.tolist()\n",
    "test_df['data_22ch_norm_crop'] = X_test_norm_cropped.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "radio-arctic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,   11,   17,   18,   20,   23,   40,   60,   79,   81,\n",
       "         88,   91,  106,  110,  123,  132,  151,  158,  166,  171,  174,\n",
       "        175,  180,  183,  222,  227,  228,  230,  234,  235,  236,  244,\n",
       "        264,  269,  290,  296,  312,  316,  331,  335,  388,  392,  395,\n",
       "        400,  419,  427,  428,  450,  460,  474,  485,  497,  509,  513,\n",
       "        517,  526,  536,  539,  544,  567,  568,  571,  572,  573,  599,\n",
       "        600,  605,  613,  614,  645,  653,  660,  668,  685,  686,  700,\n",
       "        707,  708,  711,  724,  727,  736,  741,  749,  755,  792,  794,\n",
       "        867,  870,  886,  887,  890,  896,  902,  916,  922,  936,  965,\n",
       "        974,  988,  998,  999, 1003, 1007, 1018, 1036, 1049, 1050, 1054,\n",
       "       1056, 1068, 1070, 1078, 1090, 1097, 1100, 1106, 1109, 1113, 1123,\n",
       "       1125, 1128, 1131, 1137, 1173, 1180, 1199, 1206, 1267, 1271, 1309,\n",
       "       1316, 1320, 1325, 1327, 1337, 1343, 1353, 1354, 1378, 1379, 1390,\n",
       "       1406, 1414, 1420, 1434, 1446, 1459, 1462, 1467, 1470, 1486, 1487,\n",
       "       1489, 1493, 1498, 1501, 1511, 1512, 1521, 1522, 1526, 1528, 1533,\n",
       "       1556, 1568, 1570, 1584, 1589, 1611, 1619, 1622, 1644, 1646, 1653,\n",
       "       1654, 1658, 1661, 1663, 1670, 1679, 1683])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['subject_ix']==7].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "material-deployment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1692, 500, 22)\n",
      "(423, 500, 22)\n",
      "(443, 500, 22)\n"
     ]
    }
   ],
   "source": [
    "#n_comp = 18\n",
    "\n",
    "#flip\n",
    "X_train_flipped = np.transpose(X_train_norm_cropped, (0,2,1))\n",
    "X_valid_flipped = np.transpose(X_valid_norm_cropped, (0,2,1))\n",
    "X_test_flipped = np.transpose(X_test_norm_cropped, (0,2,1))\n",
    "\n",
    "print(X_train_flipped.shape)\n",
    "print(X_valid_flipped.shape)\n",
    "print(X_test_flipped.shape)\n",
    "\n",
    "X_train_reshaped = X_train_flipped.reshape((X_train_norm_cropped.shape[0]*X_train_norm_cropped.shape[2], 22)) \n",
    "X_valid_reshaped = X_valid_flipped.reshape((X_valid_norm_cropped.shape[0]*X_valid_norm_cropped.shape[2], 22)) \n",
    "X_test_reshaped = X_test_flipped.reshape((X_test_norm_cropped.shape[0]*X_test_norm_cropped.shape[2], 22)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "meaning-duplicate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1692, 500, 15)\n",
      "(1692, 15, 500)\n",
      "(423, 15, 500)\n",
      "(443, 15, 500)\n",
      "(1692, 500, 18)\n",
      "(1692, 18, 500)\n",
      "(423, 18, 500)\n",
      "(443, 18, 500)\n",
      "(1692, 500, 20)\n",
      "(1692, 20, 500)\n",
      "(423, 20, 500)\n",
      "(443, 20, 500)\n"
     ]
    }
   ],
   "source": [
    "n_comps = [15, 18, 20]\n",
    "\n",
    "for n_comp in n_comps:\n",
    "    #try a PCA on the training data\n",
    "    pca = PCA(n_components=n_comp)                  #create the pca object       \n",
    "    pca.fit(X_train_reshaped)                               #fit it to your transformed data\n",
    "    X_train_transformed=np.empty([X_train_norm_cropped.shape[0],X_train_norm_cropped.shape[2],n_comp])\n",
    "    for i in range(len(X_train_flipped)):\n",
    "        #print(X_train_valid_flipped[i].shape)\n",
    "        X_train_transformed[i]=pca.transform(X_train_flipped[i])           #iteratively apply the transformation to each instance of the original dataset\n",
    "\n",
    "    print(X_train_transformed.shape)\n",
    "    X_train_transformed = np.transpose(X_train_transformed, (0,2,1))\n",
    "    print(X_train_transformed.shape)\n",
    "\n",
    "    #now apply that transformation to the validation and test sets\n",
    "    X_valid_transformed=np.empty([X_valid_norm_cropped.shape[0],X_valid_norm_cropped.shape[2],n_comp])\n",
    "    X_test_transformed=np.empty([X_test_norm_cropped.shape[0],X_test_norm_cropped.shape[2],n_comp])\n",
    "\n",
    "    for i in range(len(X_valid_flipped)):\n",
    "        #print(X_train_valid_flipped[i].shape)\n",
    "        X_valid_transformed[i]=pca.transform(X_valid_flipped[i])           #iteratively apply the transformation to each instance of the original dataset\n",
    "\n",
    "    #print(X_train_transformed.shape)\n",
    "    X_valid_transformed = np.transpose(X_valid_transformed, (0,2,1))\n",
    "    print(X_valid_transformed.shape)\n",
    "\n",
    "    for i in range(len(X_test_flipped)):\n",
    "        #print(X_train_valid_flipped[i].shape)\n",
    "        X_test_transformed[i]=pca.transform(X_test_flipped[i])           #iteratively apply the transformation to each instance of the original dataset\n",
    "\n",
    "    #print(X_train_transformed.shape)\n",
    "    X_test_transformed = np.transpose(X_test_transformed, (0,2,1))\n",
    "    print(X_test_transformed.shape)\n",
    "\n",
    "    train_df['data_22ch_norm_crop_pca'+str(n_comp)] = X_train_transformed.tolist()\n",
    "    valid_df['data_22ch_norm_crop_pca'+str(n_comp)] = X_valid_transformed.tolist()\n",
    "    test_df['data_22ch_norm_crop_pca'+str(n_comp)] = X_test_transformed.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "amazing-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "#examples of obtaining train_val and test dataframes from the larger dataframes\n",
    "\n",
    "s0_train_df = train_df[train_df['subject_ix'] == 0]\n",
    "s0_valid_df = valid_df[valid_df['subject_ix'] == 0]\n",
    "s0_test_df = test_df[test_df['subject_ix'] == 0]\n",
    "\n",
    "s1_train_df = train_df[train_df['subject_ix'] == 1]\n",
    "s1_valid_df = valid_df[valid_df['subject_ix'] == 1]\n",
    "s1_test_df = test_df[test_df['subject_ix'] == 1]\n",
    "\n",
    "s2_train_df = train_df[train_df['subject_ix'] == 2]\n",
    "s2_valid_df = valid_df[valid_df['subject_ix'] == 2]\n",
    "s2_test_df = test_df[test_df['subject_ix'] == 2]\n",
    "\n",
    "s3_train_df = train_df[train_df['subject_ix'] == 3]\n",
    "s3_valid_df = valid_df[valid_df['subject_ix'] == 3]\n",
    "s3_test_df = test_df[test_df['subject_ix'] == 3]\n",
    "\n",
    "s4_train_df = train_df[train_df['subject_ix'] == 4]\n",
    "s4_valid_df = valid_df[valid_df['subject_ix'] == 5]\n",
    "s4_test_df = test_df[test_df['subject_ix'] == 4]\n",
    "\n",
    "s5_train_df = train_df[train_df['subject_ix'] == 5]\n",
    "s5_valid_df = valid_df[valid_df['subject_ix'] == 5]\n",
    "s5_test_df = test_df[test_df['subject_ix'] == 5]\n",
    "\n",
    "s6_train_df = train_df[train_df['subject_ix'] == 6]\n",
    "s6_valid_df = valid_df[valid_df['subject_ix'] == 6]\n",
    "s6_test_df = test_df[test_df['subject_ix'] == 6]\n",
    "\n",
    "s7_train_df = train_df[train_df['subject_ix'] == 7]\n",
    "s7_valid_df = valid_df[valid_df['subject_ix'] == 7]\n",
    "s7_test_df = test_df[test_df['subject_ix'] == 7]\n",
    "\n",
    "s8_train_df = train_df[train_df['subject_ix'] == 8]\n",
    "s8_valid_df = valid_df[valid_df['subject_ix'] == 8]\n",
    "s8_test_df = test_df[test_df['subject_ix'] == 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "silent-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of grabbing the data from a given subject df\n",
    "s0_X_train = np.array([[[value for value in ch] for ch in run] for run in s0_train_df['data_22ch_norm_crop'].values], ndmin=3)\n",
    "s0_y_train = s0_train_df['subject_ix'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "foreign-falls",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_data(df, n_dims=22):\n",
    "    if n_dims == 22:\n",
    "        return np.array([[[value for value in ch] for ch in run] for run in df['data_22ch_norm_crop'].values], ndmin=3)\n",
    "    else:\n",
    "        return np.array([[[value for value in ch] for ch in run] for run in df['data_22ch_norm_crop_pca'+str(n_dims)].values], ndmin=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "musical-portsmouth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 22, 500)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#for example\n",
    "\n",
    "s4_X_train = get_X_data(s4_train_df, 22)\n",
    "print(s4_X_train.shape)\n",
    "\n",
    "print(type(s4_X_train[0][10][8:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "complimentary-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(n_timesteps=1000, drp=0.85, n_comp=22):\n",
    "    input_ = layers.Input(shape=(n_comp, 500))\n",
    "    r1 = layers.Reshape(target_shape=(n_comp, 500, 1))(input_)\n",
    "    c1 = layers.Conv2D(filters=40, kernel_size=(1,25), data_format='channels_last',\n",
    "                       activation='elu', kernel_regularizer='l2')(r1)\n",
    "    p1 = layers.Permute(dims=(2,1,3))(c1)\n",
    "    r2 = layers.Reshape((476, n_comp*40))(p1)\n",
    "    d1 = layers.Dense(40, activation='elu')(r2)\n",
    "    sq1 = layers.Activation(ksquare)(d1)\n",
    "    ap1 = layers.AveragePooling1D(75, strides=15)(sq1)\n",
    "    log1 = layers.Activation(klog)(ap1)\n",
    "    f1 = layers.Flatten()(log1)\n",
    "    d2 = layers.Dropout(drp)(f1)\n",
    "    output_ = layers.Dense(4, activation='softmax', kernel_regularizer='l2', kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01))(d2)\n",
    "\n",
    "    model = models.Model(inputs=input_, outputs=output_, name='shallow_pca_convnet_one_hot')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "adequate-penguin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_hist(loss_hist):\n",
    "    hist = loss_hist.history\n",
    "\n",
    "    fig, axs = plt.subplots(1,2, figsize=(20, 12))\n",
    "\n",
    "    fig.suptitle(\"Training results for dropout={} and n_dims={}\".format(drp, n_comp), fontsize=14)\n",
    "\n",
    "    axs[0].plot(hist['loss'])\n",
    "    axs[0].plot(hist['val_loss'])\n",
    "    axs[0].set_ylabel('loss')\n",
    "    axs[0].set_xlabel('epoch')\n",
    "    axs[0].legend(['train', 'val'])\n",
    "\n",
    "    axs[1].plot(hist['acc'])\n",
    "    axs[1].plot(hist['val_acc'])\n",
    "    axs[1].set_ylabel('accuracy')\n",
    "    axs[1].set_xlabel('epoch')\n",
    "    axs[1].legend(['train', 'val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "textile-tampa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-553b473d6a2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m                                          \u001b[0;31m#save_best_only=True, monitor='val_acc', mode='max')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     loss_hist = model.fit(get_X_data(s_train_df, 22), s_y_train.values, epochs=150,\n\u001b[0m\u001b[1;32m     24\u001b[0m                           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_X_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_y_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    800\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[1;32m    803\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m                **kwargs):\n\u001b[1;32m    264\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[1;32m    267\u001b[0m         sample_weights, sample_weight_modes)\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1006\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mscipy_sparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscipy_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m   \"\"\"\n\u001b[0;32m--> 261\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    262\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    268\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "#Now let's train a model on the full-scale data for all subjects\n",
    "\n",
    "for subject_ix in range(9):\n",
    "    s_train_df = train_df[train_df['subject_ix'] == subject_ix]\n",
    "    s_valid_df = valid_df[valid_df['subject_ix'] == subject_ix]\n",
    "    s_test_df = test_df[test_df['subject_ix'] == subject_ix]\n",
    "    \n",
    "    s_X_train = np.array(get_X_data(s_train_df, 22))\n",
    "    s_X_valid = get_X_data(s_valid_df, 22)\n",
    "    s_X_test = get_X_data(s_test_df, 22)\n",
    "    \n",
    "    s_y_train = s_train_df['event_class_one_hot']\n",
    "    s_y_valid = s_valid_df['event_class_one_hot']\n",
    "    s_y_test = s_test_df['event_class_one_hot']\n",
    "    \n",
    "    print(type(s_y_train.values))\n",
    "    \n",
    "    model = get_model(n_timesteps=500, drp=0.85, n_comp=22)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    #mcp_save = callbacks.ModelCheckpoint('.mdl_wts_best_acc' + str(n_comp) + '_dims.hdf5',\n",
    "                                         #save_best_only=True, monitor='val_acc', mode='max')\n",
    "    \n",
    "    loss_hist = model.fit(get_X_data(s_train_df, 22), s_y_train.values, epochs=150,\n",
    "                          validation_data=(s_X_valid, s_y_valid.values),\n",
    "                          callbacks=[], \n",
    "                          verbose=True)\n",
    "    \n",
    "    plot_train_hist(loss_hist)\n",
    "    \n",
    "    # Evaluate the model on the test data using `evaluate`\n",
    "    #print(\"Evaluate final step on test data\")\n",
    "    results = model.evaluate(s_X_test, s_y_test, batch_size=32)\n",
    "    print(\"final test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-promise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-zealand",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-breast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-finish",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-kentucky",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-reservation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-reading",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-leadership",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
