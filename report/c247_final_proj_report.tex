\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[inline]{enumitem}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
%\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{
    EEG Classification for Motor Imagery Tasks\\
    {\large ECE C247 Final Project}
}

\author{Alon Krauthammer\\
{\tt\small alonk@ucla.edu}
\and
Mark Kubiak\\
{\tt\small markkubiak@ucla.edu}
\and
Chris Munoz\\
{\tt\small cmunozcortes@ucla.edu}
\and
Eashan Samak\\
{\tt\small esamak@ucla.edu}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   The ABSTRACT is to be in fully-justified italicized text, at the top
   of the left-hand column, below the author and affiliation
   information. Use the word ``Abstract'' as the title, in 12-point
   Times, boldface type, centered relative to the column, initially
   capitalized. The abstract is to be in 10-point, single-spaced type.
   Leave two blank lines after the Abstract, then begin the main text.
   Look at previous CVPR abstracts to get a feel for style and length.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Brain-computer interfaces (BCIs) provide a direct path of communication to
measure and record brain activity. BCIs can infer human intent from these
neural activity measurements and allow individuals who have lost motor
functions gain control external devices without movement \cite{Graimann2010}.
This technique is known as motor imagery (MI).

Most BCI methods record electrical signals using electrodes
placed on the surface of the scalp (electroencephalography, EEG), or on the
surface of the cerebral cortex. EEG-based motor imagery (EEG MI) is one of the
most popular techniques in BCI due to its non-invasiness, high temporal
resolution, and portability \cite{schalk}.

Decoding of raw EEG signals for motor imagery tasks can be realized through a
variety of methods. These methods can be classified into five main categories:
\begin{enumerate*}
    \item linear classifiers (e.g. support vector machine, SVM),
    \item nonlinear Bayesian classifiers (e.g. hidden Markov models HMM),
    \item nearest neighbor classifiers (e.g. $k$-nearest neighbors, KNN),
    \item neural networks, (e.g. multi-layer perception, MLP), and
    \item combinaton of different methods using boosting or voting.
\end{enumerate*} \cite{wang}

Generally, these methods require pre-processing of the data to overcome low
signal-to-noise ratios (SNR) typical of these recordings
\cite{kostas2019machine}.
A variety of pre-processing techniques exist to extract useful features from the
data. Popular techniques include cropping, trial averaging, normalization,
band-pass filtering, and spatial transforms such as common spatial patterns
(CSP). One shortcoming of this approach is that the expert knowledge these
techniques require can introduce bias and may reinforce underlying assumptions
about the data \cite{kostas2019machine}.
In contrast, neural networks have the ability to learn representations of the
raw data without any pre-processing steps and can consider potentially unknown
correlations in the data \cite{kostas2019machine}.

In this project, we implemented three neural network architectures:
a shallow and a deep convolutional neural network based on the work by
Schirrmeister \etal in \cite{DBLP}, and an end-to-end architecture consisting of a
CNN and an LSTM layer with attention mechanism based on the work by Kostas \etal
in \cite{kostas2019machine}.

Both models were trained using the ``Four class motor imagery (001-2014)''
dataset from the BCI Competition IV (dataset 2a) \cite{brunner2008bci}. This dataset consists
of EEG data from 9 subjects who were asked to imagine moving their left hand
(class 1), right hand (class 2), both feet (class 3), and tongue (class 4).
The data contains 22 channels corresponding to twenty two electrodes that were
used to record the EEG. The signals were sampled at 250 Hz, band-pass filtered
between 0.5 Hz and 100 Hz, and notch filtered at 50 Hz for line noise
supression.
Two sessions were recorded on different days for each subject. Each session
consisted of 6 runs of 48 trials (12 for each one of the four classes), giving a
total of 288 trials per subject on each session. Specifically, the dataset
provided for this project included data for 2558 trials which we assume
corresponds to the data from one of the two sessions minus trials with bad data.
% Add the idea below to the conclusions

%In practice, however, this is difficult to achieve. The reason is that
%extracting features to express raw EEG signals in a form suitable for a neural
%network is not a trivial task. 

\section{Results}

All manuscripts must be in English.

\subsection{Data Augmentation}

\subsubsection{$\mu$ and $\beta$ Band Filtering}

% Chris 03/06: I found a paper where they use a 5th order butterworth filter to
% extract the mu band (8-13Hz) and the beta band (13-30Hz), which correspond to
% ERD and ERS (Event-related desynchronization and event-related
% synchronization) [don't ask me what those mean]. The paper trains a model with
% our data. Just throwing this out there.
% Mark: Is this it??
% https://hal.archives-ouvertes.fr/file/index/docid/837516/filename/Template.pdf

BCI research has shown that Event-Related Synchronization (ERS) and
Event-Related Desynchronization (ERD) are highly correlated with motion and have
high activity in the $\mu$ band (8Hz to 13Hz) and $\beta$ band (13Hz to 30Hz).
% TODO quote the right paper (I think the one above)
Convolutional filter sizes at the first layer were chosen to capture features of
these frequencies, but a data augmentation strategy to present the $\mu$ and
$\beta$ bands was attempted. This strategy copied the 22 EEG channels, one set
with a 6th-order Butterworth filter around the $\mu$ frequency and another set
with a 6th-order Butterworth filter around the $\beta$ frequency. The unfiltered
set and two filtered sets were concatenated to generate a $(66 \times 1000)$
sized sample. This was tried against three separate architectures - a deep CNN,
shallow CNN, and CNN+LSTM. All three models experienced either the same
performance with this augmentation or a degradation in performance. This
indicates that either these frequencies were unimportant or the model was
able to learn filters to distinguish features in these frequencies. Future work
may want to change the extra features to a measurement of power in these bands
with time.

% Another failed attempt at data augmentation was generating extra examples with
% Additive White Gaussian Noise (AWGN.) This was intended to help mitigate the
% sever overfitting issues of early deep convolutional networks. The number of
% examples was increased by a factor of 10,

\section{Discussion}

\begin{figure}[t]
\begin{center}
\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
\end{center}
   \caption{Example of caption.  It is set in Roman so that mathematics
   (always set in Roman: $B \sin A = A \sin B$) may be included without an
   ugly clash.}
\label{fig:long}
\label{fig:onecol}
\end{figure}

\section{References}

\noindent
Compare the following:\\
\begin{tabular}{ll}
 \verb'$conf_a$' &  $conf_a$ \\
 \verb'$\mathit{conf}_a$' & $\mathit{conf}_a$
\end{tabular}\\
See The \TeX book, p165.

\begin{figure*}
\begin{center}
\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
\end{center}
   \caption{Example of a short caption, which should be centered.}
\label{fig:short}
\end{figure*}

\subsection{Footnotes}


\section{References}

\begin{table}
\begin{center}
\begin{tabular}{|l|c|}
\hline
Method & Frobnability \\
\hline\hline
Theirs & Frumpy \\
Yours & Frobbly \\
Ours & Makes one's heart Frob\\
\hline
\end{tabular}
\end{center}
\caption{Results.   Ours is better.}
\end{table}

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
