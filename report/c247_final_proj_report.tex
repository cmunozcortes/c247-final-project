\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[inline]{enumitem}

\renewcommand{\ttdefault}{cmtt}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
%\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{
    EEG Classification for Motor Imagery Tasks\\
    {\large ECE C247 Final Project}
}

\author{Alon Krauthammer\\
{\tt\small alonk@ucla.edu}
\and
Mark Kubiak\\
{\tt\small markkubiak@ucla.edu}
\and
Chris Munoz\\
{\tt\small cmunozcortes@ucla.edu}
\and
Eashan Samak\\
{\tt\small esamak@ucla.edu}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   The ABSTRACT is to be in fully-justified italicized text, at the top
   of the left-hand column, below the author and affiliation
   information. Use the word ``Abstract'' as the title, in 12-point
   Times, boldface type, centered relative to the column, initially
   capitalized. The abstract is to be in 10-point, single-spaced type.
   Leave two blank lines after the Abstract, then begin the main text.
   Look at previous CVPR abstracts to get a feel for style and length.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
% TODO remove background on BCIs and the EEG classification problem per project
% guidelines
Brain-computer interfaces (BCIs) provide a direct path of communication to
measure and record brain activity. BCIs can infer human intent from these
neural activity measurements and allow individuals who have lost motor
functions gain control external devices without movement \cite{Graimann2010}.
This technique is known as motor imagery (MI).
Most BCI methods record electrical signals using electrodes
placed on the surface of the scalp (electroencephalography, EEG), or on the
surface of the cerebral cortex. EEG-based motor imagery (EEG MI) is one of the
most popular techniques in BCI due to its non-invasiness, high temporal
resolution, and portability \cite{schalk}.
Decoding of raw EEG signals for motor imagery tasks can be realized through a
variety of methods. These methods can be classified into five main categories:
\begin{enumerate*}
    \item linear classifiers (e.g. support vector machine, SVM),
    \item nonlinear Bayesian classifiers (e.g. hidden Markov models HMM),
    \item nearest neighbor classifiers (e.g. $k$-nearest neighbors, KNN),
    \item neural networks, (e.g. multi-layer perception, MLP), and
    \item combination of different methods using boosting or voting.
\end{enumerate*} \cite{wang}

Generally, these methods require pre-processing of the data to overcome low
signal-to-noise ratios (SNR) typical of these recordings
\cite{kostas2019machine}.
A variety of pre-processing techniques exist to extract useful features from the
data. Popular techniques include cropping, trial averaging, normalization,
band-pass filtering, and spatial transforms such as common spatial patterns
(CSP). One shortcoming of this approach is that the expert knowledge these
techniques require can introduce bias and may reinforce underlying assumptions
about the data \cite{kostas2019machine}.
In contrast, neural networks have the ability to learn representations of the
raw data without any pre-processing steps and can consider potentially unknown
correlations in the data \cite{kostas2019machine}.

In this project, we implemented several neural network architectures to examine
their performance on EEG classification, and attempted several pre-processing
techniques to improve performance of one of the models.

The first model implemented was a shallow convolutional neural network
based on the work by Schirrmeister \etal in \cite{DBLP}. There were two main
reasons for implementing this model: first, it provided a strong performance
baseline for comparison with subsequent models without requiring any manual
feature engineering; and second, it could be augmented with other kinds of
layers to enable us to explore more complex architectures.
% TODO: Alon - can you add a description of your model here and correct
% my description if it's wrong?

% Chris: Commenting this out - I thought we had implemented a model based on the
% deep CNN from the Schirrmeister paper, but it turns out we didn't
%The second model we implemented was based on the deep CNN architecture in
%\cite{DBLP}. This model was described by its authors as implementing a generic
%architecture designed only with minor expert knowledge and capable of extracting
%a wide range of features. With that in mind, we implemented a slightly modified
%version of this model with the purpose of comparing its performance with its
%shallow counterpart.

The third model we implemented was a spatial convolutional neural network
(SCNN) designed by Kostas \etal \cite{kostas2019machine}. This architecture
includes multiple layers of spatial convolutions and temporal convolutions.
These two types of layers are implemented separately so that spatial features
are extracted independently from temporal features. The idea behind this is
mimicking a typical feature-based pipeline consisting of spatial channel mixing
and band-pass filtering to extract useful features.

The last model we examined was an augmented version of the SCNN described above
that added an LSTM layer and attention mechanism after the spatial and temporal
convolutional layers. The goal of the LSTM layer with attention mechanism is to
enable the LSTM to develop a sequential processing capability with flexibility
to consider any combination of samples that may be appropriate, rather than
processing the sequence temporally in a sample by sample fashion
\cite{kostas2019machine}.

% Add the idea below to the conclusions

%In practice, however, this is difficult to achieve. The reason is that
%extracting features to express raw EEG signals in a form suitable for a neural
%network is not a trivial task. 

\section{Results}
All models were trained using the ``Four class motor imagery (001-2014)''
dataset from the BCI Competition IV (dataset 2a) \cite{brunner2008bci}.
The data contains 22 channels corresponding to twenty two electrodes that were
used to record the EEG. The signals were sampled at 250 Hz, band-pass filtered
between 0.5 Hz and 100 Hz, and notch filtered at 50 Hz for line noise
suppression.
Two sessions were recorded on different days for each subject. Each session
consisted of 6 runs of 48 trials (12 for each one of the four classes), giving a
total of 288 trials per subject on each session. Specifically, the dataset
provided for this project included data for 2558 trials which we assume
corresponds to the data from one of the two sessions minus trials with bad data.

In the first part of our analysis, we evaluated the performance of each model on
the task of classification across all subjects. To this end, we split the
training dataset with 80\% used for training and the remaining 20\% for
validation.

The validation accuracy and test accuracy for each model is shown in Table
\ref{tab:acc} for the three models evaluated. Considering that, according to
\cite{wang},  EEG data is in general subject dependent, these results are very
reasonable and right in line with the classification accuracy reported in
\cite{kostas2019machine} and \cite{DBLP}.

\section{Discussion}
\subsection{Data Augmentation}

\subsubsection{$\mu$ and $\beta$ Band Filtering}

% Chris 03/06: I found a paper where they use a 5th order butterworth filter to
% extract the mu band (8-13Hz) and the beta band (13-30Hz), which correspond to
% ERD and ERS (Event-related desynchronization and event-related
% synchronization) [don't ask me what those mean]. The paper trains a model with
% our data. Just throwing this out there.
% Mark: Is this it??
% https://hal.archives-ouvertes.fr/file/index/docid/837516/filename/Template.pdf
% Chris: Yes, that's the one. Also, I'm thinking that maybe the details of the
% data augmentation can go in the extra page (aka the Methods section) and keep
% the discussion about the effects of each technique in the Discussion section.

BCI research has shown that Event-Related Synchronization (ERS) and
Event-Related Desynchronization (ERD) are highly correlated with motion and have
high activity in the $\mu$ band (8Hz to 13Hz) and $\beta$ band (13Hz to 30Hz)
\cite{yang2013subject}.
Convolutional filter sizes at the first layer were chosen to capture features of
these frequencies, but a data augmentation strategy to present the $\mu$ and
$\beta$ bands was attempted. This strategy copied the 22 EEG channels, one set
with a 6th-order Butterworth filter around the $\mu$ frequency and another set
with a 6th-order Butterworth filter around the $\beta$ frequency. The unfiltered
set and two filtered sets were concatenated to generate a $(66 \times 1000)$
sized sample. This was tried against three separate architectures - a deep CNN,
shallow CNN, and CNN+LSTM. All three models experienced either the same
performance with this augmentation or a degradation in performance. This
indicates that either these frequencies were unimportant or the model was
able to learn filters to distinguish features in these frequencies. Future work
may want to change the extra features to a measurement of power in these bands
with time.

% Another failed attempt at data augmentation was generating extra examples with
% Additive White Gaussian Noise (AWGN.) This was intended to help mitigate the
% sever overfitting issues of early deep convolutional networks. The number of
% examples was increased by a factor of 10,


\begin{figure}[t]
\begin{center}
\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
\end{center}
   \caption{Example of caption.  It is set in Roman so that mathematics
   (always set in Roman: $B \sin A = A \sin B$) may be included without an
   ugly clash.}
\label{fig:long}
\label{fig:onecol}
\end{figure}

\begin{figure*}
\begin{center}
\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
\end{center}
   \caption{Example of a short caption, which should be centered.}
\label{fig:short}
\end{figure*}

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
