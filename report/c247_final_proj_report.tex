\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[inline]{enumitem}

\renewcommand{\ttdefault}{cmtt}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
%\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{
    EEG Classification for Motor Imagery Tasks\\
    {\large ECE C247 Final Project}
}

\author{Alon Krauthammer\\
{\tt\small alonk@ucla.edu}
\and
Mark Kubiak\\
{\tt\small markkubiak@ucla.edu}
\and
Chris Munoz\\
{\tt\small cmunozcortes@ucla.edu}
\and
Eashan Samak\\
{\tt\small esamak@ucla.edu}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
    % TODO write abstract!
   The ABSTRACT is to be in fully-justified italicized text, at the top
   of the left-hand column, below the author and affiliation
   information. Use the word ``Abstract'' as the title, in 12-point
   Times, boldface type, centered relative to the column, initially
   capitalized. The abstract is to be in 10-point, single-spaced type.
   Leave two blank lines after the Abstract, then begin the main text.
   Look at previous CVPR abstracts to get a feel for style and length.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
%Brain-computer interfaces (BCIs) provide a direct path of communication to
%measure and record brain activity. BCIs can infer human intent from these
%neural activity measurements and allow individuals who have lost motor
%functions gain control external devices without movement \cite{Graimann2010}.
%This technique is known as motor imagery (MI).
%Most BCI methods record electrical signals using electrodes
%placed on the surface of the scalp (electroencephalography, EEG), or on the
%surface of the cerebral cortex. EEG-based motor imagery (EEG MI) is one of the
%most popular techniques in BCI due to its non-invasiness, high temporal
%resolution, and portability \cite{schalk}.
Decoding of raw EEG signals for motor imagery tasks can be realized through a
variety of methods. These methods can be classified into five main categories:
\begin{enumerate*}
    \item linear classifiers (e.g. support vector machine, SVM),
    \item nonlinear Bayesian classifiers (e.g. hidden Markov models HMM),
    \item nearest neighbor classifiers (e.g. $k$-nearest neighbors, KNN),
    \item neural networks, (e.g. multi-layer perception, MLP), and
    \item combination of different methods using boosting or voting.
\end{enumerate*} \cite{wang}

Generally, these methods require pre-processing of the data to overcome low
signal-to-noise ratios (SNR) typical of these recordings
\cite{kostas2019machine}.
A variety of pre-processing techniques exist to extract useful features from the
data. Popular techniques include cropping, trial averaging, normalization,
band-pass filtering, and spatial transforms such as common spatial patterns
(CSP). One shortcoming of this approach is that the expert knowledge these
techniques require can introduce bias and may reinforce underlying assumptions
about the data.
In contrast, neural networks have the ability to learn representations of the
raw data without any pre-processing steps and can consider potentially unknown
correlations in the data \cite{kostas2019machine}.

In this project, we implemented several neural network architectures to examine
their performance on EEG classification, and attempted several pre-processing
techniques to verify whether performance could be improved if the models were
fed handcrafted features as opposed to raw EEG data.
Our ultimate goal was to analyze the performance of three different models on
the task of subject independent classification (i.e. across all subjects on the
dataset), and to analyze the performance of at least one model on the task of
subject-dependent classification. 

The first model implemented was a shallow convolutional neural network
based on the work by Schirrmeister \etal in \cite{DBLP}. There were two main
reasons for implementing this model: first, it provided a strong performance
baseline for comparison with subsequent models without requiring any manual
feature engineering; and second, it could be augmented with other kinds of
layers to enable us to explore more complex architectures.
% TODO: Alon - can you add a description of your model here and correct
% my description if it's wrong?

% Chris: Commenting this out - I thought we had implemented a model based on the
% deep CNN from the Schirrmeister paper, but it turns out we didn't
%The second model we implemented was based on the deep CNN architecture in
%\cite{DBLP}. This model was described by its authors as implementing a generic
%architecture designed only with minor expert knowledge and capable of extracting
%a wide range of features. With that in mind, we implemented a slightly modified
%version of this model with the purpose of comparing its performance with its
%shallow counterpart.

The third model we implemented was a spatial convolutional neural network
(SCNN) designed by Kostas \etal \cite{kostas2019machine}. This architecture
includes multiple layers of spatial convolutions and temporal convolutions.
These two types of layers are implemented separately so that spatial features
are extracted independently from temporal features. The idea behind this is
mimicking a typical feature-based pipeline consisting of spatial channel mixing
and band-pass filtering.

The last model we examined was an augmented version of the SCNN described above
that added an LSTM layer and attention mechanism after the spatial and temporal
convolutional layers. The goal of the LSTM layer with attention mechanism is to
enable the LSTM to develop a sequential processing capability with flexibility
to consider any combination of samples that may be appropriate, rather than
processing the sequence temporally in a sample by sample fashion
\cite{kostas2019machine}.

% Add the idea below to the conclusions

%In practice, however, this is difficult to achieve. The reason is that
%extracting features to express raw EEG signals in a form suitable for a neural
%network is not a trivial task. 

\section{Results}
All models were trained using the ``Four class motor imagery (001-2014)''
dataset from the BCI Competition IV (dataset 2a) \cite{brunner2008bci}.
The data contains 22 channels corresponding to twenty two electrodes that were
placed on the surface of the subjects' scalp.
The signals were sampled at 250 Hz, band-pass filtered between 0.5 Hz and 100
Hz, and notch filtered at 50 Hz for line noise suppression.
Two sessions were recorded on different days for each subject. Each session
consisted of 6 runs of 48 trials (12 for each one of the four classes), giving a
total of 288 trials per subject on each session. Specifically, the dataset
provided for this project included data for 2558 trials which we assume
corresponds to the data from one of the two sessions minus trials with bad data.

In the first part of our analysis, we evaluated the performance of each model on
the task of classification across all subjects. To this end, we split the
training dataset with 80\% used for training and the remaining 20\% for
validation.

In general, the accuracy of the models we
implemented ranged from 65\% to 75\% on the test set. Considering that EEG data
is in general subject dependent \cite{wang}, these results are very reasonable
and right in line with the classification accuracy reported in
\cite{kostas2019machine} and \cite{DBLP} when attempting to classify the data
across all subjects on this dataset. Similarly, our results using the shallow
CNN to perform subject specific classification are in line with the performance
reported by other neural network models on this dataset \cite{wang}.

The validation accuracy and test accuracy results for each model are included
on a separate document, together with plots of the training and validation loss
and accuracy over number of epochs.

\section{Discussion}
\subsection{Pre-processing and Data Augmentation}
We attempted several pre-processing steps and data augmentation techniques to
try to improve the baseline performance of our shallow CNN model (60\%).
These steps included input data standardization, cropping in time,
dimensionality reduction, and band-pass filtering in the $\mu$ and $\beta$
bands.

\subsubsection{Input Standardization}
The first notable result was that standardization of the inputs seemed to have a
regularizing effect on the given shallow CNN model, which was initially
overfitting quite badly. The standardization that was attempted was as follows:
for each example, calculate the mean and variance for each of the 22 channels. 
Then, mean-center each channel and scale to unit variance. This was performed as
a preprocessing step rather than a regularization step, so these adjustments
were explicitly calculated for each of the training, validation, and test
examples. This had a clear regularizing effect, reducing overfitting and
allowing for meaningful learning to occur for an additional ~10 training epochs,
improving performance by roughly 2-3\%.

\subsubsection{Cropping}
The next pre-processing step that was found to be helpful was cropping the
examples in time. With the full 1000 samples representing 4 seconds of data, we
tried using only the first 3 seconds, as well as the last 3 seconds. It was
clear from the performance of these two cropped models that the first second of
data was very important, and indeed the final 1-2 seconds of each recording were
having a slight negative impact on model performance. The best performance came
from using only the first two seconds, or 500 samples, of each recording. This
also reduced the size of the fully-connected layers, which may have further
reduced overfitting and thereby been responsible for the marginally increased
performance.

\subsubsection{Dimensionality reduction}
Additionally, we sought to pre-process the data with a PCA, reducing the number
of channels and distilling the useful information for better model performance.
An iterative search was performed on the standardized, cropped dataset,
reducing the number of channels from 22 to $n$, where $n$ was varied between 6
and 21. 
This was performed in a nested fashion with a search over values of $p$ for
dropout between 0.7 and 0.9. Models were trained for 150 epochs. It was found
that PCA modestly improved model performance in general when reducing to
between 12 and 20 channels. Best test and validation accuracy was generally
found for 16-18 channels, but the overall best test accuracy of 73.14\% was
achieved by a model reducing to 20 channels in pre-processing. 
Overall, the contribution of PCA to test performance was roughly 3-4\%.
More typical test accuracy for this model was 71-72\%.


\subsubsection{$\mu$ and $\beta$ Band Filtering}

% Chris 03/06: I found a paper where they use a 5th order butterworth filter to
% extract the mu band (8-13Hz) and the beta band (13-30Hz), which correspond to
% ERD and ERS (Event-related desynchronization and event-related
% synchronization) [don't ask me what those mean]. The paper trains a model with
% our data. Just throwing this out there.
% Mark: Is this it??
% https://hal.archives-ouvertes.fr/file/index/docid/837516/filename/Template.pdf

BCI research has shown that Event-Related Synchronization (ERS) and
Event-Related Desynchronization (ERD) are highly correlated with motion and have
high activity in the $\mu$ band (8Hz to 13Hz) and $\beta$ band (13Hz to 30Hz)
\cite{yang2013subject}.
Convolutional filter sizes at the first layer were chosen to capture features of
these frequencies, but a data augmentation strategy to present the $\mu$ and
$\beta$ bands was attempted. This strategy copied the 22 EEG channels, one set
with a 6th-order Butterworth filter around the $\mu$ frequency and another set
with a 6th-order Butterworth filter around the $\beta$ frequency. The unfiltered
set and two filtered sets were concatenated to generate a $(66 \times 1000)$
sized sample. This was tried against three separate architectures - a deep CNN,
shallow CNN, and CNN+LSTM. All three models experienced either the same
performance with this augmentation or a degradation in performance. This
indicates that either these frequencies were unimportant or the model was
able to learn filters to distinguish features in these frequencies. Future work
may want to change the extra features to a measurement of power in these bands
with time.

% Another failed attempt at data augmentation was generating extra examples with
% Additive White Gaussian Noise (AWGN.) This was intended to help mitigate the
% sever overfitting issues of early deep convolutional networks. The number of
% examples was increased by a factor of 10,

\subsection{Model Performance}
% TODO add discussion about the shallow CNN and its performance
A significant part of our efforts were directed towards improving the
performance of our shallow CNN model, which reached a baseline test accuracy of
~60\% on the task of subject-independent classification. 
We believe most of the improvement over the baseline came from the
pre-processing steps described on the previous sections, and regularization
coupled with hyperparameter tuning, which we will discuss here.
Adding dropout with a value of $p=0.8$ prior to the dense
layer at the output (see additional \emph{Model} section for architecture
details) had the greatest impact in reducing overfitting. 
We added another dropout layer after the first fully-connected layer with a
value of $p=0.6$, but this did not yield any measurable gains.
Other items that did not yield meaningful improvements, and in some cases
resulted in worse performance, include different choice of activation function
and adding L1/L2 regularization.

In addition to performing classification across all subjects in the dataset, we
used this model to perform subject-specific classification. We reached two
relevant conclusions from this work. First, that model performance varied
greatly between subjects, with subjects 2 and 6 having the lowest accuracy. When
removing subjects 2 and 6 from the overall dataset to train a model, this
improved performance by ~4\% on average.
Second, that standardization, which improved performance of the overall model,
actually hindered performance on the per-subject models. We hypothesized that
this is because the models were learning consistent differences in amplitudes
per action for a given subject, but these differences were not always preserved
from subject to subject.

The last investigation we pursued using this model was multi-task training on
action classification and subject classification. This was motivated by the
per-subject findings above. However, a model trained on subject classification
only, even with the standardization performed in the overall model above,
achieved 99.8\% test accuracy, and similar validation accuracy after only a
handful of training epochs. This experiment was not pursued due to the apparent
triviality of subject classification on this dataset. 

The second model that we implemented, the spatial convolutional neural network
(SCNN), reached lower accuracy than the shallow CNN on the task of
subject-independent classification; however, we did not spend a significant
amount of time tuning hyperparameters and we only trained it with raw EEG data
and no data augmentation techniques whatsoever.
Considering this, the performance of this model was still reasonable, correctly
classifying over 65\% of the test dataset.
There were several hyperparameters we would have tuned had we had more compute
power available and more time. Some of them would have been the number of
filters on the spatial convolutional layers, the number of filters on the
temporal convolutional layers, the dropout factor, and the learning rate.

The SCNN+LSTM with attention mechanism was the best performing model on the task
of subject independent classification. 
This result can be explained by two architectural decisions. 
First, the inclusion of separate spatial convolutional layers and temporal
convolutional layers, which allows the model to learn useful spatial features
independently from temporal features. 
Second, the inclusion of the attention mechanism, which allows the LSTM layer to
learn what parts of the input sequence are more relevant to the task at hand. 
Kostas \etal \cite{kostas2019machine} report a mean accuracy of 73.3\% with a
standard deviation of 13.6\%, so our accuracy of 75\% falls within the range
reported by authors of the model.
Amongst the hyperparameters we tuned for this model were dropout factor, choice
of activation function, learning rate, and batch size.

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
